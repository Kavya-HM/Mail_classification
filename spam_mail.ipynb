{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8712dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96c2c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9b7361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6678d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62dffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889f3fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING THE DATASET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"PREPROCESSING THE DATASET\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24c1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Perform stemming or lemmatization\n",
    "    porter = nltk.stem.porter.PorterStemmer()\n",
    "    tokens = [porter.stem(token) for token in tokens]\n",
    "    \n",
    "    # Rejoin the tokens into a string\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6562a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the text column\n",
    "data[\"text\"] = data[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091c357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data to a new CSV file\n",
    "data.to_csv(\"preprocessed_spam.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbed5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLITTING THE DATASET\n"
     ]
    }
   ],
   "source": [
    "print(\"SPLITTING THE DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c940d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef10b0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data is 5728\n",
      "Index(['text', 'spam'], dtype='object')\n",
      "                                                text  spam\n",
      "0  subject natur irresist corpor ident lt realli ...     1\n",
      "1  subject stock trade gunsling fanni merril muzo...     1\n",
      "2  subject unbeliev new home made easi im want sh...     1\n",
      "3  subject 4 color print special request addit in...     1\n",
      "4  subject money get softwar cd softwar compat gr...     1\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed CSV file\n",
    "data = pd.read_csv('preprocessed_spam.csv')\n",
    "print(\"Length of the data is\",len(data))\n",
    "print(data.columns)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f50a4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5728</td>\n",
       "      <td>5728.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5511</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>subject term project list project member quant...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.238827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.426404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text         spam\n",
       "count                                                5728  5728.000000\n",
       "unique                                               5511          NaN\n",
       "top     subject term project list project member quant...          NaN\n",
       "freq                                                    2          NaN\n",
       "mean                                                  NaN     0.238827\n",
       "std                                                   NaN     0.426404\n",
       "min                                                   NaN     0.000000\n",
       "25%                                                   NaN     0.000000\n",
       "50%                                                   NaN     0.000000\n",
       "75%                                                   NaN     0.000000\n",
       "max                                                   NaN     1.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c43d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['spam'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d103a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text data into a matrix of token counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58bbfb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING THE MODEL\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING THE MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff92063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier on the token count matrix\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d773de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test data\n",
    "y_pred = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c09d8299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9851657940663177\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "521cf735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[849   7]\n",
      " [ 10 280]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae4ff6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       856\n",
      "           1       0.98      0.97      0.97       290\n",
      "\n",
      "    accuracy                           0.99      1146\n",
      "   macro avg       0.98      0.98      0.98      1146\n",
      "weighted avg       0.99      0.99      0.99      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efa5da55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be63d953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: ['ham', 'spam']\n"
     ]
    }
   ],
   "source": [
    "new_messages = [\n",
    "    \"Hi Mark, can you send me that report?\",\n",
    "    \"You've won a free trip to Dubai!\"\n",
    "]\n",
    "\n",
    "new_messages = pd.Series(new_messages)\n",
    "new_messages = new_messages.apply(preprocess_text)\n",
    "new_messages_counts = vectorizer.transform(new_messages)\n",
    "new_messages_pred = clf.predict(new_messages_counts)\n",
    "new_messages_pred = ['spam' if label == 1 else 'ham' for label in new_messages_pred]\n",
    "print(\"Predicted Labels:\", new_messages_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be72669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
